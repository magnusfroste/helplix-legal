import { useCallback, useEffect, useMemo } from 'react';
import type { CooperSettings, ConversationStatus } from '@/types/helplix';
import { useRealtimeVoice } from './useRealtimeVoice';
import { useCooperChat } from './useCooperChat';
import { usePhaseTracking } from './usePhaseTracking';
import { useLogEntries } from './useLogEntries';
import { toast } from 'sonner';

interface UseConversationOptions {
  settings: CooperSettings;
  userId?: string;
}

export function useConversation({ settings, userId }: UseConversationOptions) {
  // Log entries and session management
  const logEntries = useLogEntries({
    settings,
    userId,
    onError: (error) => toast.error(error),
  });

  // Phase and quality tracking
  const phaseTracking = usePhaseTracking();

  // Voice service
  const voice = useRealtimeVoice();

  // Chat service
  const chat = useCooperChat({
    settings,
    currentPhase: phaseTracking.currentPhase,
    informationGaps: phaseTracking.informationGaps,
    completeness: phaseTracking.completeness,
    onError: (error) => toast.error(error),
  });

  // Derived status - memoized to prevent recalculation
  const status: ConversationStatus = useMemo(() => {
    if (voice.isRecording) return 'listening';
    if (voice.isTranscribing) return 'processing';
    if (chat.isLoading) return 'thinking';
    if (voice.isSpeaking) return 'speaking';
    return 'idle';
  }, [voice.isRecording, voice.isTranscribing, chat.isLoading, voice.isSpeaking]);

  const isBusy = useMemo(() => status !== 'idle', [status]);

  // Sync language to session
  useEffect(() => {
    if (chat.detectedLanguage) {
      logEntries.updateSessionLanguage(chat.detectedLanguage);
    }
  }, [chat.detectedLanguage, logEntries.updateSessionLanguage]);

  // Core action: process user response
  const processResponse = useCallback(async (text: string) => {
    console.log('processResponse called with:', { 
      text, 
      userId, 
      currentSessionId: logEntries.currentSessionId, 
      phase: phaseTracking.currentPhase 
    });
    
    // Save to log entries
    const { savedQuestion, savedAnswer } = await logEntries.addQuestionAndAnswer(
      logEntries.currentQuestion,
      text
    );

    if (!savedQuestion || !savedAnswer) {
      return; // Failed to save, error already shown
    }

    // Process through phase tracking to get follow-up decisions
    const { shouldFollowUp, followUpQuestion, nextPhase } = phaseTracking.processUserResponse(
      text,
      logEntries.currentQuestion
    );

    try {
      let nextQuestion: string;
      
      if (shouldFollowUp && followUpQuestion) {
        // Use the follow-up question generated by quality control
        nextQuestion = followUpQuestion;
      } else {
        // Get next question from AI
        nextQuestion = await chat.sendMessage(text, nextPhase);
      }
      
      logEntries.updateCurrentQuestion(nextQuestion);

      if (settings.autoplaySpeech && settings.ttsEnabled) {
        voice.speak(nextQuestion).catch(console.error);
      }
    } catch (error) {
      console.error('AI response error:', error);
    }
  }, [userId, logEntries, phaseTracking, settings.autoplaySpeech, settings.ttsEnabled, chat, voice]);

  // Actions
  const startRecording = useCallback(async () => {
    if (isBusy) return;
    
    try {
      await voice.startRecording();
    } catch (error) {
      console.error('Failed to start recording:', error);
      toast.error('Failed to start recording');
    }
  }, [isBusy, voice]);

  const stopRecording = useCallback(async () => {
    // Optimistic UI: Show placeholder immediately
    const optimisticId = logEntries.addOptimisticEntry();

    try {
      const text = await voice.stopRecording();
      
      // Remove optimistic entry
      logEntries.removeOptimisticEntry(optimisticId);
      
      if (!text.trim()) {
        toast.error('No speech detected');
        return;
      }
      await processResponse(text);
    } catch {
      // Remove optimistic entry on error
      logEntries.removeOptimisticEntry(optimisticId);
      toast.error('Failed to process recording');
    }
  }, [voice, logEntries, processResponse]);

  const submitText = useCallback(async (text: string) => {
    await processResponse(text);
  }, [processResponse]);

  const replayQuestion = useCallback(() => {
    if (!settings.ttsEnabled) {
      toast.error('Text-to-speech is disabled');
      return;
    }
    if (logEntries.currentQuestion && !voice.isSpeaking) {
      voice.speak(logEntries.currentQuestion).catch(console.error);
    }
  }, [logEntries.currentQuestion, voice, settings.ttsEnabled]);

  const startNewSession = useCallback(async () => {
    try {
      await logEntries.startNewSession();
      chat.resetConversation();
      phaseTracking.reset();
      
      toast.success('New session started');

      if (settings.autoplaySpeech && settings.ttsEnabled) {
        voice.speak(logEntries.currentQuestion).catch(console.error);
      }
    } catch {
      console.error('Failed to start new session');
    }
  }, [logEntries, chat, phaseTracking, settings.autoplaySpeech, settings.ttsEnabled, voice]);

  return {
    // State (read-only)
    status,
    isBusy,
    currentQuestion: logEntries.currentQuestion,
    isFirstInteraction: logEntries.isFirstInteraction,
    logEntries: logEntries.logEntries,
    audioLevel: voice.audioLevel,
    currentSessionId: logEntries.currentSessionId,
    phaseProgress: phaseTracking.phaseProgress,
    infoTracker: phaseTracking.infoTracker,
    qualityMetrics: phaseTracking.qualityMetrics,
    lastAssessment: phaseTracking.lastAssessment,
    realtimeTranscriptionText: '', // Placeholder - can be added to voice hook if needed

    // Actions
    startRecording,
    stopRecording,
    submitText,
    replayQuestion,
    startNewSession,

    // Pass-through for Report
    speak: voice.speak,
    stopSpeaking: voice.stopSpeaking,
    isSpeaking: voice.isSpeaking,
  };
}
